# Advanced Search Engine with Langchain Mistral

## Overview
## Optimization with Groq

To optimize performance and reduce latency, this search engine leverages Groq, a specialized API for querying Langchain models. Groq provides efficient access to Langchain Mistral, enabling faster response times and smoother user interactions.

By integrating Groq into the search engine, we ensure that users experience minimal delays when querying the language model, resulting in a more responsive and seamless search experience.



This project is an advanced search engine powered by Langchain Mistral, a state-of-the-art language model. It utilizes cutting-edge natural language processing (NLP) techniques to provide users with accurate and relevant search results.

## Features

- **AI-Powered Search**: Utilizes Langchain Mistral to understand and interpret user queries.
- **Customizable LLM Selection**: Allows users to choose from different Langchain Mistral models for their specific needs.
- **Conversational Memory**: Maintains conversational context to provide more relevant search results over time.

## Usage

1. Clone the repository:



2. Install dependencies:


3. Run the application:

4. Access the application in your browser:

- Local URL: [http://localhost:8501](http://localhost:8501)
- Network URL: [http://192.168.164.151:8501](http://192.168.164.151:8501)

## Configuration



- `groq_api_key`: Your Groq API key for accessing Mistral.
- `default_model`: The default Langchain Mistral model to use.
- `conversational_memory_length`: The length of conversational memory to maintain.

For any questions or issues, please open an [issue](https://github.com/your_username/your_repo/issues) on GitHub.




This is the python code developed in advanced and with support to latency speed in deploying in AI models wrt inferencing speed of LLM's while maintaining model performance.

 The exciting process of building one of the fastest AI chatbots using Groq Chat, powered by the revolutionary Tensor Streaming Processor (TSP) technology developed by Groq. This technology enables unparalleled processing speeds for machine learning workloads, especially for complex Natural Language Processing (NLP) tasks. Groq Chat, built atop this cutting-edge hardware, stands out by providing real-time responses to natural language queries, making it an exceptional tool for a myriad of applications including customer service, technical support, and sales.
 delve into how to utilize Streamlit to create a user-friendly interface for interacting with Groq Chat. By integrating a Large Language Model, Mixtral 8x7b, and leveraging LangChain for advanced functionalities, I showcase the process of developing a chatbot that not only understands and responds to user queries with incredible speed but also handles a vast volume of interactions simultaneously without breaking a sweat.
 #

Whether you're aiming to enhance customer engagement, provide swift technical support, or boost your sales efforts through real-time interactions, this tutorial equips you with the knowledge to leverage the power of Groq's TSP technology. 


Error issue:

Regarding the deprecation warning, it seems that the __call__ function in the LangChain library is being deprecated and will be removed in a future version. You should update your code to use the recommended invoke function inste
